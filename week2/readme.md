# Week 2 실습 노트 (Supervised & Unsupervised Learning)

`week2_practice.ipynb` 노트북의 내용을 정리한 문서입니다.  
지도학습(회귀·분류)과 비지도학습(차원축소·군집)을 한 번에 실습해 볼 수 있도록 구성되어 있습니다.

---

## 1. 실습 목표

- 지도학습(Supervised Learning)의 기본 흐름 이해
  - 데이터 전처리 → 학습/테스트 분할 → 모델 학습 → 평가·시각화
- 회귀(Regression)와 분류(Classification)의 차이와 대표 모델 비교
- 비지도학습(Unsupervised Learning)에서 차원축소(PCA)와 군집(K-Means)의 개념·동작 방식 체험
- 하이퍼파라미터 변화에 따른 성능 변화를 직접 실험

---

## 2. 사용 라이브러리

- 데이터 처리: `numpy`, `pandas`
- 시각화: `matplotlib`
- 머신러닝 모델 및 전처리: `scikit-learn`
  - `LinearRegression`, `RandomForestRegressor`
  - `LogisticRegression`, `DecisionTreeClassifier`
  - `train_test_split`, `StandardScaler`
  - `PCA`, `KMeans`
  - `mean_squared_error`, `r2_score`, `accuracy_score`
  - `confusion_matrix`, `ConfusionMatrixDisplay`
- 데이터셋
  - `load_diabetes`
  - `load_breast_cancer`
  - `load_digits`

---

## 3. 노트북 구성

### 3.1 지도학습 - 회귀 (Regression)

**목표**

- 연속적인 값을 예측하는 회귀 문제를 통해 전처리 → 학습 → 평가 전체 흐름 익히기
- 선형 모델과 앙상블 트리 모델의 성능을 비교해 보기

**내용**

- 데이터: `load_diabetes` (당뇨병 진행 정도 회귀 데이터)
- EDA
  - `info()`, `describe()`를 이용한 기본 탐색
  - 타깃 값 분포 히스토그램 시각화
- 결측치 처리 예시
  - 0이 나올 수 없는 변수에서 0을 측정 실패로 가정
  - 0을 `NaN`으로 바꾸고, `dropna()`로 결측치 행 제거
- 데이터 분할 및 스케일링
  - `train_test_split`으로 학습/테스트 데이터 분리
  - `StandardScaler`로 특성 스케일 조정

#### 3.1.1 Linear Regression (선형 회귀)

- 가정:  
  - 타깃 y가 입력 특성의 **선형 결합**으로 표현 가능.
  - 식:  
    - ŷ = wᵀx + b
- **목표**:  
  - 실제값 y와 예측값 ŷ 사이의 오차를 최소화하는 w, b를 찾는 것  
  - 보통 **최소제곱(Least Squares)** 방식 사용.
- **장점**
  - 계산이 빠르고 구현이 단순.
  - 가중치 w를 통해 각 특성의 영향력 해석 가능.
- **단점**
  - 관계가 비선형이면 표현력이 부족.
  - 이상치(outlier)에 민감.

#### 3.1.2 Random Forest Regressor (랜덤 포레스트 회귀)

- 여러 개의 **결정 트리 회귀 모델**을 학습하고, 그 예측값을 **평균**해서 사용하는 **앙상블 모델**.
- 구조
  - 각 트리는 데이터와 특성을 **부분 샘플링**해서 학습 (bagging).
  - 예측 시 여러 트리의 결과를 평균.
- **장점**
  - 비선형 관계, 복잡한 패턴도 어느 정도 학습 가능.
  - 개별 트리보다 과적합이 줄어들고 안정적.
- **단점**
  - 해석이 어렵다(왜 이런 예측을 했는지 직관적으로 보기 힘듦).
  - 트리 개수 많으면 학습·예측 비용 증가.

#### 3.1.3 MSE(Mean Squared Error, 평균제곱오차) 개념 정리

**1. 무엇을 나타내는 지표인가?**

- 회귀(regression) 문제에서 **모델 예측값이 실제값과 얼마나 차이가 나는지**를 나타내는 대표적인 손실 함수/평가지표.
- “오차의 크기”를 **제곱해서 평균낸 값**이라고 이해하면 됨.

**2. 수식**

- 데이터가 n개 있고,
  - 실제값: yᵢ  
  - 예측값: ŷᵢ  
  라고 할 때,

\[
\mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

- `(실제값 − 예측값)`을 제곱해서 모두 더한 뒤, **평균**을 내는 구조.

**3. 해석**

- **값이 작을수록** 모델 예측이 실제값에 가깝다는 뜻.
- 단위는 원래 타깃의 단위를 **제곱한 단위**
  - 예: 높이(m)를 예측하는 문제면 MSE의 단위는 m².
- 큰 오차를 제곱하므로,
  - **큰 오차(Outlier)** 에 더 큰 패널티를 주는 특징이 있음.

**4. 특징·장단점**

- **장점**
  - 미분이 가능해서, 경사하강법 같은 최적화 알고리즘에 쓰기 좋음.
  - 수학적으로 다루기 편하고, 이론적으로 많이 연구된 손실 함수.
- **단점**
  - 단위가 제곱이라 직관적으로 “얼마나 틀렸는지” 감이 잘 안 올 수 있음 →  
    그래서 종종 **RMSE(√MSE)** 를 함께 봄.
  - 이상치가 있으면 그 영향이 매우 크게 반영됨.

**5. 한 줄 요약**

> MSE는 **“예측값과 실제값의 차이를 제곱해서 평균낸 값”**으로,  
> 값이 작을수록 예측이 정확하며, **큰 오차를 특히 강하게 벌주는 회귀용 손실/평가지표**이다.

#### 3.1.4 R² (결정계수)

- **모델이 타깃의 분산을 얼마나 설명하는지**를 나타내는 지표.
- R² = 1이면 데이터 분산을 완벽히 설명, 0이면 “평균만 쓰는 모델과 비슷”,  
  0보다 작으면 평균보다도 못한 모델이라는 뜻.

---

### 3.2 지도학습 - 분류 (Classification)

**목표**

- 이진 분류 문제를 통해 로지스틱 회귀와 결정 트리 모델 비교
- 정확도와 혼동행렬을 통해 분류 성능 평가 방법 이해

**내용**

- 데이터: `load_breast_cancer` (유방암 진단 데이터)
- 타깃 분포 확인 및 `stratify` 옵션을 사용한 균형 있는 train/test 분할
- 결측치 여부 확인

#### 3.2.1 Logistic Regression (로지스틱 회귀)

- 이름에 “회귀”가 붙지만, 실제로는 **이진 분류 모델**.
- 선형 결합 z = wᵀx + b 를 **시그모이드 함수**에 통과:
  - σ(z) = 1 / (1 + e⁻ᶻ)
  - σ(z)는 0~1 사이의 값 → **“클래스 1일 확률”**로 해석.
- 예측 규칙
  - σ(z) ≥ 0.5 → 클래스 1
  - σ(z) < 0.5 → 클래스 0
- **스케일링 중요**
  - `StandardScaler`로 먼저 정규화하는 것이 일반적.

#### 3.2.2 Decision Tree Classifier (결정 트리)

- 데이터를 **질문(조건)**으로 계속 나누어 가는 나무(Tree) 구조 모델.
- 하이퍼파라미터
  - `max_depth`: 트리의 최대 깊이 → 과적합/과소적합 조절.
- **장점**
  - 규칙이 “if ~ else” 형태라 결과 해석이 쉽다.
  - 특성 스케일링 필요 없음.
- **단점**
  - 깊이 제한 없으면 과적합이 매우 심해질 수 있음.

#### 3.2.3 분류 평가 지표

- **Accuracy (정확도)**  
  - 전체 샘플 중에서 **정답 맞춘 비율**.
- **Confusion Matrix (혼동행렬)**
  - TP, TN, FP, FN 구조로 어떤 클래스에서 많이 틀리는지 확인 가능.
  - `ConfusionMatrixDisplay`로 시각화.

---

### 3.3 비지도학습 - 차원축소 & 군집

**목표**

- 레이블 없이 데이터 패턴을 찾는 비지도학습의 기본 흐름 이해
- 고차원 데이터를 저차원으로 축소해 시각화하는 방법 학습
- 군집 알고리즘이 어떤 식으로 그룹을 나누는지 체험

**데이터**

- `load_digits`
  - 8×8 픽셀(64차원) 손글씨 숫자 데이터
  - 실제 숫자 라벨(0~9)은 비지도학습에서 정답으로 사용하지 않고, 시각화 시 색깔 비교용으로만 활용

#### 3.3.1 PCA (Principal Component Analysis, 주성분 분석)

- 목적
  - 고차원 데이터(예: 64차원 이미지)를 **정보 손실 최소화하면서 저차원**으로 줄이기.
  - 데이터 분산을 가장 잘 설명하는 **새로운 축(주성분)**을 찾는다.
- 핵심 개념
  - 데이터 분산을 가장 많이 설명하는 방향부터 순서대로 축을 선택.
  - `explained_variance_ratio_`로 각 주성분이 전체 분산 중 얼마나 설명하는지 확인.
- 시각화
  - 64차원 손글씨 숫자를 PCA 2차원으로 줄인 후, 실제 숫자 라벨(0~9)을 색으로 표시하여 군집 구조 확인.

#### 3.3.2 K-Means Clustering

- 목표
  - 데이터에서 **K개의 중심(centroid)**를 찾고, 각 데이터 포인트를 가장 가까운 중심에 할당하여 군집을 만든다.
- 기본 단계
  1. 초기 중심 K개를 임의로 설정.
  2. 각 포인트를 가장 가까운 중심에 할당.
  3. 각 군집의 평균으로 중심 업데이트.
  4. 수렴할 때까지 2–3단계 반복.
- 실습
  - PCA 2차원 공간에서 `n_clusters=10`으로 K-Means 수행.
  - 색을 클러스터 번호로 표시하여 군집 결과 시각화.

---

## 4. 마무리 및 실험 가이드

노트북에서 실습한 내용:

1. **지도학습 - 회귀**
   - 0을 결측치로 간주하여 처리하는 예시
   - Linear Regression vs Random Forest Regression
   - MSE, R²로 성능 비교 및 실제값-예측값 산점도 시각화

2. **지도학습 - 분류**
   - Logistic Regression (이진 분류, 확률 출력)
   - Decision Tree Classifier (나무 모양 모델, 해석 용이)
   - 정확도 & 혼동행렬로 성능 비교

3. **비지도학습**
   - PCA로 64차원 손글씨 데이터를 2차원으로 축소해 시각화
   - K-Means로 레이블 없이 군집화 수행

